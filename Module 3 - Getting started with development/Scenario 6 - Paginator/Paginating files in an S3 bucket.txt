		Paginating files in an S3 bucket - Section1 uploading files to bucket

1- Create a 100 text files in a folder

for lp in range(120):
    filename = "C:\\Users\\aminouri\\Documents\\Coding\\Files\\file" + str(lp) + ".txt"
    file = open(filename, "x")

2- Zip the files before uploading them to s3
3- Create an S3 bucket

import boto3
client = boto3.client('s3')
client.create_bucket(Bucket='report-am-many-objects', CreateBucketConfiguration={'LocationConstraint': 'us-west-2'})

4- Create an IAM role for the lambda function responsible for unzipping the file
*	Provide ‘AmazonS3FullAccess’ and ‘AWSLambdaBasicExecutionRole’ for the role

5- Create the lambda function to get triggered when uploading the zip file and unzip all the files
*	After creating the function with the IAM role created in step 4, add trigger as follows
*	Event type: PUT, Suffix: .zip
*	Add the following code to your lambda function

Note: Do not add Prefix in this example. For some reason it won't work!

import boto3
import string
import random
import os
import zipfile
def lambda_handler(event, context):
    s3_resource= boto3.resource('s3')
    s3_client= boto3.client('s3')
    bucketName = event['Records'][0]['s3']['bucket']['name']
    bucket = s3_resource.Bucket(bucketName)
    zip_key = event['Records'][0]['s3']['object']['key']
    
    
    chars=string.ascii_uppercase + string.digits
    randomName = ''.join(random.choice(chars) for _ in range(8))
    tmpFolder = '/tmp/' +  randomName + '/'
    os.makedirs(tmpFolder)
    unzipTmpFile= randomName + '.zip'
    attachmentFolder=''
    extension = ".zip"
    targetDirectory = 'Myfiles-folder'
    
    s3_client.download_file(bucketName, zip_key, tmpFolder + unzipTmpFile)
    dir_name = tmpFolder
    os.chdir(dir_name)
    
    for item in os.listdir(tmpFolder):
      if item.endswith(extension):
        file_name = os.path.abspath(item)
        zip_ref = zipfile.ZipFile(file_name) 
        zip_ref.extractall(dir_name) 
        zip_ref.close() 
        os.remove(file_name)
        
    mrssFiles = []
    # r=root, d=directories, f = files
    for r, d, f in os.walk(tmpFolder):
      for file in f:
        mrssFiles.append(os.path.join(r, file))
    for file_name in mrssFiles:
      s3_client.upload_file(file_name, bucketName, targetDirectory + '/' + file_name.replace(tmpFolder, '', 1))
      os.remove(file_name)
    return {
        'statusCode': 200,
        'body': zip_key
    }


5- Upload the zip file to the S3 bucket to see a new folder created and files extracted to it.

import boto3
client = boto3.client('s3')
response = client.upload_file(Filename='C:\\Users\\aminouri\\Documents\\Coding\\Files\\All-Files.zip', Bucket='report-am-many-objects', Key='All-Files.zip')
print(response)

6- Run the following code to show you 10 files in each page or query. Chnage the value to see different results

import boto3
client = boto3.client('s3')
count = 0
paginator = client.get_paginator('list_objects')
page_iterator = paginator.paginate(Bucket = 'report-am-many-objects', PaginationConfig={'MaxItems': 10})
for page in page_iterator:
    for lp in range(100):
        count+=1
        print(count, page['Contents'][lp]['Key'])


Ref: https://princekfrancis.medium.com/multiple-file-upload-into-amazon-s3-bucket-9888d51001bb